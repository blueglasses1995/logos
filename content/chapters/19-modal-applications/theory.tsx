import {
  Callout,
  FormulaBlock,
  ComparisonTable,
  KeyPoint,
  SectionDivider,
  MotivationSection,
} from "@/components/content"
import {
  ExampleMapping,
  InlineMiniQuiz,
  StudyNotes,
} from "@/components/interactive"

export function TheoryContent() {
  return (
    <>
    <article className="prose prose-zinc max-w-none">
      <h1>第22章: 様相論理の応用</h1>

      <MotivationSection
        icon="🧠"
        realWorldExample="マルチエージェントAIの「このエージェントは何を知っているか」「ロボットは何をすべきか」は、認識論理・義務論理で形式化される。"
        nextChapterConnection="時相論理で「時間」の概念を論理に組み込む"
      />

      <h2>認識論理: 「知っている」を形式化する</h2>

      <p>
        前章で学んだ様相演算子□を「知っている」と解釈すると、
        <strong>認識論理（epistemic logic）</strong>が得られます。
        マルチエージェントシステムや分散コンピューティングで、
        各エージェントが何を知っているかを厳密に推論するための体系です。
      </p>

      <Callout variant="definition" label="認識演算子">
        <strong>K_a P</strong>:「エージェントaはPを知っている」{"\n\n"}
        クリプキモデルで解釈すると:{"\n"}
        M, w ⊨ K_a P ⟺ エージェントaにとってwと区別できないすべての世界w'で、Pが真
      </Callout>

      <p>
        「知っている」の直感を形式化するポイントは、
        「エージェントにとって区別できない世界」という概念です。
        エージェントaの情報が限られていると、
        現実がw₀であってもw₁であっても区別がつかない場合があります。
        そのような「区別できないすべての世界」でPが成り立つとき、
        aは「Pを知っている」と言えます。
      </p>

      <FormulaBlock caption="知識の基本性質">
        K_a P → P （真実性: 知っていることは真である）{"\n"}
        K_a P → K_a K_a P （内省: 知っていることを知っている）{"\n"}
        ¬K_a P → K_a ¬K_a P （否定的内省: 知らないことを知っている）
      </FormulaBlock>

      <Callout variant="warning" label="知識と信念の違い">
        知識の論理では K_a P → P（知っていることは真）が成り立ちますが、
        信念の論理では B_a P → P は成り立ちません。
        人は偽のことを信じることができるからです。
        これは前章の公理Tに対応する問題です。
      </Callout>

      <ExampleMapping
        formula="K_a P ∧ ¬K_b P"
        example="アリスはパスワードを知っているが、ボブは知らない"
        variables={{ "K_a": "アリスは知っている", "K_b": "ボブは知っている", "P": "パスワード" }}
      />

      <InlineMiniQuiz
        question="「太郎はこの定理の証明を知っている」を認識論理で表すと？"
        options={["K_太郎 P", "◇P", "□P", "P → K_太郎 P"]}
        correctIndex={0}
        explanation="K_a Pは「エージェントaはPを知っている」の表記です。ここでa=太郎、P=「この定理の証明」です。"
      />

      <KeyPoint>
        認識論理は様相論理の□を「知っている」と解釈したもの。
        到達可能性関係は「エージェントにとって区別できない世界」の関係に対応する。
      </KeyPoint>

      <SectionDivider />

      <h2>共有知識と共通知識</h2>

      <p>
        マルチエージェントの文脈では、「みんなが知っている」ことと
        「みんなが知っていることをみんなが知っている」ことは異なります。
        この区別は、分散システムの合意アルゴリズムにおいて決定的に重要です。
      </p>

      <Callout variant="definition" label="共有知識と共通知識">
        <strong>共有知識（mutual knowledge）E_G P</strong>:{"\n"}
        グループGの全員がPを知っている。{"\n"}
        E_G P = K_a P ∧ K_b P ∧ K_c P ∧ ...{"\n\n"}
        <strong>共通知識（common knowledge）C_G P</strong>:{"\n"}
        全員がPを知っていて、全員が「全員がPを知っている」ことを知っていて、
        全員が「全員が『全員がPを知っている』ことを知っている」ことを知っていて...
        （無限の入れ子）
      </Callout>

      <ComparisonTable
        headers={["共有知識 E_G P", "共通知識 C_G P"]}
        rows={[
          ["全員がPを知っている", "全員がPを知っていることを、全員が知っている（無限の入れ子）"],
          ["1段階の知識", "無限段階の知識"],
          ["メールで情報共有した状態", "全員が同じ部屋でアナウンスを聞いた状態"],
          ["合意の必要条件", "合意の十分条件"],
        ]}
      />

      <Callout variant="example" label="メール vs 全員集合">
        3人のチームにメールで「明日のミーティングは10時」と伝えた場合。{"\n\n"}
        共有知識: 全員がメールを読めば、全員が10時を知っている。{"\n"}
        しかし: アリスは「ボブがメールを読んだか」を知らないかもしれない。{"\n"}
        つまりK_Alice K_Bob P は不確実。{"\n\n"}
        全員がいる部屋でアナウンスした場合:{"\n"}
        共通知識: 全員が聞き、全員が「全員が聞いた」ことを見ており、
        全員が「全員が『全員が聞いた』ことを見た」ことも見ている...
      </Callout>

      <InlineMiniQuiz
        question="分散システムで全ノードが同時に行動を開始するために必要なのは？"
        options={["共通知識", "共有知識", "一つのノードの知識", "知識は不要"]}
        correctIndex={0}
        explanation="分散合意には共通知識が必要です。全員が「全員が合意した」ことを知り、さらにそれを全員が知っている...という無限の入れ子が、同時行動の根拠になります。"
      />

      <KeyPoint>
        共有知識（全員が知っている）と共通知識（知っていることの知識が無限に入れ子）の
        区別は、分散システムの合意プロトコルの理論的基盤である。
      </KeyPoint>

      <SectionDivider />

      <h2>義務論理: 「すべき」を形式化する</h2>

      <p>
        様相演算子を「義務」と解釈すると、
        <strong>義務論理（deontic logic）</strong>が得られます。
        法律、倫理、AIの行動規範を形式的に推論するための体系です。
      </p>

      <Callout variant="definition" label="義務演算子">
        <strong>O(P)</strong>:「Pは義務的である」（Pすべきである）{"\n"}
        <strong>P(P)</strong>:「Pは許容される」（Pしてもよい）{"\n"}
        <strong>F(P)</strong>:「Pは禁止である」（Pしてはならない）
      </Callout>

      <FormulaBlock caption="義務演算子の関係">
        O(P) ⟺ ¬P(¬P) （義務 ⟺ しないことが許されない）{"\n"}
        F(P) ⟺ O(¬P) （禁止 ⟺ しない義務がある）{"\n"}
        P(P) ⟺ ¬O(¬P) （許容 ⟺ しない義務がない）
      </FormulaBlock>

      <ComparisonTable
        headers={["義務論理", "様相論理（通常）"]}
        rows={[
          ["O(P): Pは義務的", "□P: Pは必然的"],
          ["P(P): Pは許容", "◇P: Pは可能"],
          ["O(P) ⟺ ¬P(¬P)", "□P ⟺ ¬◇¬P"],
          ["O(P) → P は成り立たない！", "□P → P（公理T）"],
        ]}
      />

      <Callout variant="warning" label="義務論理の注意点">
        義務論理では O(P) → P が成り立ちません。
        「Pすべきである」からといって「Pが実際に行われている」とは限りません。
        法律が存在しても違反は起きます。
        これは、義務論理の到達可能性関係が反射的でないことに対応します。
      </Callout>

      <ExampleMapping
        formula="O(通知する) ∧ ¬通知した → 違反"
        example="GDPRでは個人データの漏洩を通知する義務がある。通知しなければ違反となる。"
        variables={{ "O": "義務がある", "P": "データ漏洩を通知する" }}
      />

      <InlineMiniQuiz
        question="「個人情報を無断で第三者に提供してはならない」を義務論理で表すと？"
        options={["F(P) すなわち O(¬P)", "O(P)", "P(P)", "¬O(P)"]}
        correctIndex={0}
        explanation="「してはならない」は禁止F(P)であり、これは「しない義務」O(¬P)と同値です。"
      />

      <SectionDivider />

      <h2>認識論理のパズル: マディ・チルドレン問題</h2>

      <p>
        認識論理の力を示す有名なパズルがあります。
        このパズルは、共通知識の役割を劇的に示します。
      </p>

      <Callout variant="example" label="マディ・チルドレン問題">
        3人の子供が遊んでいます。そのうち2人の額に泥がついています。{"\n"}
        各自は他の子の額は見えますが、自分の額は見えません。{"\n\n"}
        父親が「少なくとも1人の額に泥がついている」と宣言します。{"\n"}
        そして「自分の額に泥がついていると分かった人は手を挙げなさい」と繰り返し尋ねます。{"\n\n"}
        1回目: 誰も手を挙げない。{"\n"}
        2回目: 泥のついた2人が手を挙げる。{"\n\n"}
        なぜ？
      </Callout>

      <p>
        このパズルの鍵は、父親の宣言が<strong>共通知識</strong>を生み出すことにあります。
        実は、2人に泥がついていることは宣言前から全員が知っていました（共有知識）。
        しかし、「全員がそれを知っていることを全員が知っている」状態ではなかったのです。
      </p>

      <p>
        泥のついた子供A（額に泥あり）の推論:{"\n"}
        1回目: 「もし自分に泥がなければ、Bだけに泥がある。
        するとBは他に泥を見ないので、父の宣言を聞いてすぐ手を挙げるはず。
        Bが手を挙げなかった。ゆえに自分にも泥がある。」{"\n"}
        2回目: Aは手を挙げる。同じ推論でBも手を挙げる。
      </p>

      <KeyPoint>
        マディ・チルドレン問題は、共有知識と共通知識の違いが実際の推論に影響を与えることを示す。
        父親の公開宣言が共通知識を生み出し、各エージェントの推論を進める触媒となる。
      </KeyPoint>

      <SectionDivider />

      <h2>日常推論での様相: 法的議論と倫理的議論</h2>

      <p>
        様相論理は抽象的な理論に見えますが、
        法律や倫理の推論で日常的に使われる概念を形式化しています。
      </p>

      <ComparisonTable
        headers={["分野", "様相的表現", "論理的解釈"]}
        rows={[
          ["法律", "「〜しなければならない」「〜してもよい」", "O(P), P(P)"],
          ["倫理", "「〜すべきである」「〜は許されない」", "O(P), F(P)"],
          ["契約", "「甲は〜する義務を負う」", "O_甲(P)"],
          ["知的財産", "「〜を知っていた」「〜を知るべきだった」", "K_a P, O(K_a P)"],
          ["プライバシー", "「〜を知ってはならない」", "F(K_a P)"],
        ]}
      />

      <Callout variant="example" label="法的推論の例">
        <strong>過失の判断</strong>:{"\n"}
        O(K_a(危険)) ∧ ¬K_a(危険) → 過失{"\n"}
        「aは危険を知るべきであった」かつ「aは実際に知らなかった」なら過失。{"\n\n"}
        <strong>善意の第三者</strong>:{"\n"}
        ¬K_c(瑕疵) → 保護される{"\n"}
        「第三者cが瑕疵を知らなかった」なら保護される。
      </Callout>

      <InlineMiniQuiz
        question="「医師は患者にリスクを説明する義務がある」を形式化すると？"
        options={[
          "O(医師がリスクを説明する)",
          "K_医師(リスク)",
          "◇(医師がリスクを説明する)",
          "□(医師がリスクを説明する)"
        ]}
        correctIndex={0}
        explanation="「義務がある」は義務演算子O()で表します。K_医師(リスク)は「医師がリスクを知っている」で、説明の義務ではありません。"
      />

      <SectionDivider />

      <h2>様相論理とAI: 知識表現と信念改訂</h2>

      <p>
        現代のAI研究において、様相論理は知識表現の基盤的なツールです。
        特にマルチエージェントシステムでは、
        各エージェントの知識・信念・意図を形式的に表現し推論する必要があります。
      </p>

      <ComparisonTable
        headers={["AI分野", "関連する様相論理", "応用例"]}
        rows={[
          ["知識表現", "認識論理 K_a P", "エージェントが何を知っているか"],
          ["プランニング", "信念-欲求-意図(BDI)論理", "ロボットの行動計画"],
          ["信念改訂", "動的認識論理", "新情報に基づく信念の更新"],
          ["マルチエージェント", "共通知識 C_G P", "合意形成プロトコル"],
          ["AI倫理", "義務論理 O(P)", "AIの行動規範の形式化"],
        ]}
      />

      <Callout variant="example" label="BDIアーキテクチャ">
        自律エージェントの設計で広く使われるBDI（Belief-Desire-Intention）モデル:{"\n\n"}
        <strong>Belief（信念）</strong>: B_a P — エージェントaがPを信じている{"\n"}
        <strong>Desire（欲求）</strong>: D_a P — エージェントaがPを望んでいる{"\n"}
        <strong>Intention（意図）</strong>: I_a P — エージェントaがPを実行するつもりである{"\n\n"}
        例: ロボットが「部屋が散らかっている」と信じ（B）、
        「部屋をきれいにしたい」と望み（D）、
        「掃除を実行する」と意図する（I）。
      </Callout>

      <KeyPoint>
        様相論理はAIの知識表現・信念改訂・マルチエージェント合意の理論的基盤である。
        認識論理は分散システムの不可能性定理（合意不可能性）の証明にも使われ、
        理論と実践の両面で重要な役割を果たしている。
      </KeyPoint>
    </article>
    <div className="not-prose my-8">
      <StudyNotes chapterSlug="19-modal-applications" />
    </div>
    </>
  )
}
